{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    # Define test directory and create results folder\n",
    "    test_dir = \"data/processed/test\"\n",
    "    results_dir = \"results\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Define class names\n",
    "    class_names = ['Bacterial Leaf Blight', 'Brown Spot', 'Leaf Smut']\n",
    "\n",
    "    # Create test data generator\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    try:\n",
    "        model = tf.keras.models.load_model('models/finetuned_model_v2.h5')\n",
    "    except:\n",
    "        print(\"Failed to load finetuned_model_v2.h5, trying finetuned_model.h5\")\n",
    "        try:\n",
    "            model = tf.keras.models.load_model('models/finetuned_model.h5')\n",
    "        except:\n",
    "            print(\"Failed to load finetuned_model.h5, trying trained_model.h5\")\n",
    "            model = tf.keras.models.load_model('models/trained_model.h5')\n",
    "\n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(test_generator)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = test_generator.classes\n",
    "\n",
    "    # Compute classification report\n",
    "    report = classification_report(true_classes, predicted_classes, target_names=class_names, output_dict=True)\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
    "\n",
    "    # Save metrics to text file\n",
    "    with open(os.path.join(results_dir, 'metrics.txt'), 'w') as f:\n",
    "        f.write(f\"Test Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(f\"{'Class':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\\n\")\n",
    "        f.write(\"-\" * 65 + \"\\n\")\n",
    "        for cls in class_names:\n",
    "            f.write(f\"{cls:<25} {report[cls]['precision']:<10.2f} {report[cls]['recall']:<10.2f} \"\n",
    "                    f\"{report[cls]['f1-score']:<10.2f} {int(report[cls]['support']):<10}\\n\")\n",
    "        f.write(\"-\" * 65 + \"\\n\")\n",
    "        f.write(f\"{'Macro Avg':<25} {report['macro avg']['precision']:<10.2f} {report['macro avg']['recall']:<10.2f} \"\n",
    "                f\"{report['macro avg']['f1-score']:<10.2f} {int(report['macro avg']['support']):<10}\\n\")\n",
    "        f.write(f\"{'Weighted Avg':<25} {report['weighted avg']['precision']:<10.2f} {report['weighted avg']['recall']:<10.2f} \"\n",
    "                f\"{report['weighted avg']['f1-score']:<10.2f} {int(report['weighted avg']['support']):<10}\\n\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot bar plots for precision, recall, and F1-score\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    for metric in metrics:\n",
    "        values = [report[cls][metric] for cls in class_names]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        bars = plt.bar(class_names, values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "        plt.title(f'{metric.capitalize()} per Class')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.ylim(0, 1)\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, value + 0.01, f'{value:.2f}', ha='center')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'{metric}_bar_plot.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
